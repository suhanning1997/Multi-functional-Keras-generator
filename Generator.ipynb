{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_sg = KeyedVectors.load('word2vec_sg.wordvectors')\n",
    "\n",
    "def matrix_generator(dataframe, list_IDs_temp, min_len = 1, max_len = 100, dim = (100, 200)):\n",
    "    '''\n",
    "    Input:\n",
    "    filename: name of the csv file we want to read data from\n",
    "    list_TDs_temp: indeces for minibatch\n",
    "    min_len: minimum number of tokenized words for a title + abstraction text\n",
    "    max_len: maximum number of tokenized words for a title + abstraction text\n",
    "    dim: dimension of outputing matrix representation of text\n",
    "    \n",
    "    Output:\n",
    "    X: batch of matrix representations of num_doc number of title+abstraction texts\n",
    "    y: labels\n",
    "    '''\n",
    "    batch_size = len(list_IDs_temp)\n",
    "    X = np.zeros((batch_size, *dim))\n",
    "    y = np.zeros((1, batch_size), dtype = int)\n",
    "    counter = 0\n",
    "    \n",
    "    for idx in list_IDs_temp:\n",
    "        if dataframe.iloc[idx][\"abstract\"] == 'NULL':\n",
    "            line = re.sub('[^a-zA-Z0-9]', ' ', str(dataframe.iloc[idx][\"title\"])) # remove non-letters and non-numbers\n",
    "            bool_list = [word in word_vectors_sg for word in utils.simple_preprocess(line)]\n",
    "            filtered_line = list(compress(utils.simple_preprocess(line), bool_list)) #some infrequency words are not included in the vocab, \n",
    "            #we remove then\n",
    "            length = len(filtered_line)\n",
    "            if length >= min_len:\n",
    "                matrix = word_vectors_sg[filtered_line] # generate document matrix representation from tokens\n",
    "                X_row = np.pad(matrix, ((0, max_len - length),(0, 0))).reshape([*dim])\n",
    "                X[counter] = X_row\n",
    "                y[0, counter] = dataframe.iloc[idx][\"label\"] - 1\n",
    "        else:\n",
    "            line = re.sub('[^a-zA-Z0-9]', ' ', str(dataframe.iloc[idx][\"title\"]) + ' ' + str(dataframe.iloc[idx][\"abstract\"])) # remove non-letters and non-numbers\n",
    "            bool_list = [word in word_vectors_sg for word in utils.simple_preprocess(line)]\n",
    "            filtered_line = list(compress(utils.simple_preprocess(line), bool_list)) #some infrequent words are not included in the vocab, \n",
    "            #we remove then\n",
    "            length = len(filtered_line)\n",
    "            if length >= min_len and length <= max_len:\n",
    "                matrix = word_vectors_sg[filtered_line] # generate document matrix representation from tokens\n",
    "                X_row = np.pad(matrix, ((0, max_len - length),(0, 0))).reshape([*dim])\n",
    "                X[counter] = X_row\n",
    "                y[0, counter] = dataframe.iloc[idx][\"label\"] - 1\n",
    "            elif length > max_len:\n",
    "                matrix = word_vectors_sg[filtered_line[0:max_len]] # generate document matrix representation from tokens\n",
    "                X_row = matrix.reshape([*dim])\n",
    "                X[counter] = X_row\n",
    "                y[0, counter] = dataframe.iloc[idx][\"label\"] - 1\n",
    "        counter += 1\n",
    "    return X, np.squeeze(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model training\n",
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, dataframe, batch_size=100, text_length = 100,\n",
    "                 n_classes=125, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dataframe = dataframe\n",
    "        self.dim = (text_length, 200)\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list(range(len(dataframe.loc[:, \"patent_id\"])))\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.dataframe.loc[:, \"patent_id\"]) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        X, y = matrix_generator(self.dataframe, list_IDs_temp)\n",
    "        return X, to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For prediction assessments\n",
    "class DataGenerator2(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, dataframe, batch_size=100, text_length = 100,\n",
    "                 n_classes=125, shuffle=False):\n",
    "        'Initialization'\n",
    "        self.dataframe = dataframe\n",
    "        self.dim = (text_length, 200)\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list(range(len(dataframe.loc[:, \"patent_id\"])))\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.n = 0\n",
    "        self.max = self.__len__()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.dataframe.loc[:, \"patent_id\"]) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        _, y = matrix_generator(self.dataframe, list_IDs_temp)\n",
    "        return to_categorical(y, num_classes=self.n_classes)\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.n >= self.max:\n",
    "           self.n = 0\n",
    "        y = self.__getitem__(self.n)\n",
    "        self.n += 1\n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
